---
layout: post
title: tensorflow和CNN学习笔记1--基本概念
---
## 1.为什么是tensorflow？
当前流行的深度学习框架很多，主流的有caffe、Tensorflow、MXnet、Torch等等，学术界用caffe比较多，MXnet的性能可能更好一些，Tensorflow算是中规中矩，但Tensorflow因为有了google的支持，当属目前最流行的。选择框架和选择一门语言类似，越流行其获得的支持就越多，其bug就会越少，遇到问题能够获得的帮助信息也就越多。

## 2.什么是CNN？
CNN中文是卷积神经网络，主要用来处理图片数据。这里的卷积和通常的图像处理的卷积不一样的地方在于，通常图像处理中用到的卷积都是高斯等固定的卷积核，而CNN中每个卷积核的值都是需要我们训练才得的的，使用卷积代替之前神经网络中的全连接，它的好处在于相对于全连接来说，卷积操作通过共享权重的方式极大的降低了参数的数量，同时也捕获到相邻像素点之间的关联信息。

## 3.什么是Tensor？
Tensor中文译做张量，是一个数学名词，简单来说，单个数值（标量）是0阶张量，一行数值（向量）是1阶张量，矩阵是2阶张量，立方体则是3阶张量...通常CNN的主要应用对象是图片，而图片一般是RGB3通道或RGBA4通道，再加上min batch，这样的训练数据其实是一个4阶张量，包括模型的weight和中间结果feature map都是4阶张量，可以想象CNN的训练过程就是数据（Tensor）在网络中流动（flow）的过程，这大概就是google把这样一个深度学习框架命名为Tensorflow的原因吧。

## 4.惰性计算
惰性计算意思就是你写得代码不是立即就被执行的，而是等到有数据输入的时候你的代码才真正被执行了，不知道这样的理解是不是正确，不正确的话欢迎指正。我第一次接触惰性计算行的概念还是从学习scala开始的，scala也是先构造一个计算流，然后等数据输入的时候才真正开始计算，scala只负责检查数据类型是否正确，至于具体的数据长什么样，事先是无法得知的。类似的，在Tensorflow里，所有的数据处理操作和网络结构其实都是你定义的一个graph中的节点，只有当给定了一个sess并feed数据的时候，程序才会开始执行，因此一些数据只有在真正执行中才能获取，比如说我想搭建一个CNN网络结构能适应所有大小的图片，就可以事先不给定图片的长宽，直接在执行过程中获取，我们可以用placeholder的方法占位，真正执行的时候feed数据到placeholder就可以了。比如这样定义的运算并不会立马执行，而需要给定a和b之后才会真正开始计算c

```python
a = tf.placeholder(tf.float32)
b = tf.placeholder(tf.float32)
c = a + b
```

当然python本身是不支持这种动态获取方式的，有一些额外的非属于tensorflow的处理操作就需要提前给定具体的值了，比如我想对batch的数据扫一遍，python的for循环中就必须事先就给定batch_size的大小而不能动态的获取。当我们实际使用过程中还会遇到一些让人迷惑的操作，比如tf.shape和tensor数据本身的shape方法，这两者的差异也其实就代表了动态计算和静态计算，后面我会给出具体解释。
